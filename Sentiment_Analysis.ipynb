{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayurshimbre123/mayurshimbre/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "474d63ca",
      "metadata": {
        "id": "474d63ca"
      },
      "source": [
        "**Name** - Mayur Shimbre\n",
        "\n",
        "**Roll Number** - BE18B007\n",
        "\n",
        "**Institution** - Indian Institute of Technology Madras\n",
        "\n",
        "**Project Name** - Sentiment Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "296fc61d",
      "metadata": {
        "id": "296fc61d"
      },
      "source": [
        "## 1. Loading Libraries and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9de8e68",
      "metadata": {
        "id": "f9de8e68"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import twitter_samples, stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129b1368",
      "metadata": {
        "id": "129b1368"
      },
      "source": [
        "### 1.1 Data Preparation & Preprocessing\n",
        "\n",
        "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets. \n",
        "\n",
        "    - If I used all three datasets, I would introduce duplicates of the positive tweets and negative tweets.  \n",
        "    - So, I will select just the five thousand positive tweets and five thousand negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "405e5b11",
      "metadata": {
        "id": "405e5b11"
      },
      "outputs": [],
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c53226df",
      "metadata": {
        "id": "c53226df"
      },
      "source": [
        "* Train test split: 20% will be in the test set, and 80% in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97f015d5",
      "metadata": {
        "id": "97f015d5"
      },
      "outputs": [],
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set) \n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg \n",
        "test_x = test_pos + test_neg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b881555",
      "metadata": {
        "id": "9b881555"
      },
      "source": [
        "* Numpy array of positive labels and negative labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "333bfe54",
      "metadata": {
        "id": "333bfe54"
      },
      "outputs": [],
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e83f3d98",
      "metadata": {
        "id": "e83f3d98",
        "outputId": "19b55b29-5a5f-4c45-afaf-c784b8c64d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ]
        }
      ],
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13515fe",
      "metadata": {
        "id": "c13515fe"
      },
      "source": [
        "* The following function `process_tweet()` tokenizes the tweet into individual words, removes stop words and applies stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d11149",
      "metadata": {
        "id": "29d11149"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00fbb75f",
      "metadata": {
        "id": "00fbb75f",
        "outputId": "245c6c64-221d-4d02-f913-f70c2748688b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is an example of a positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ],
      "source": [
        "# Testing `process_tweet()` function\n",
        "print('This is an example of a positive tweet: \\n', train_x[0])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7efc3c24",
      "metadata": {
        "id": "7efc3c24"
      },
      "source": [
        "* I have created the frequency dictionary using the following `build_freqs()` function.  \n",
        "     \n",
        "* The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0).  The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c967b95",
      "metadata": {
        "id": "5c967b95"
      },
      "outputs": [],
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99b5a433",
      "metadata": {
        "id": "99b5a433",
        "outputId": "67bebd44-65da-4f2e-8a5b-70b3d09d5124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11339\n"
          ]
        }
      ],
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fdd74b2",
      "metadata": {
        "id": "9fdd74b2"
      },
      "source": [
        "## 2: Logistic regression Model\n",
        "\n",
        "\n",
        "### 2.1: Sigmoid\n",
        "\n",
        "* The sigmoid function is defined as: \n",
        "\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
        "\n",
        "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80d17d10",
      "metadata": {
        "id": "80d17d10"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z): \n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    \n",
        "    # calculate the sigmoid of z\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "        \n",
        "    return h"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c69de5e",
      "metadata": {
        "id": "7c69de5e"
      },
      "source": [
        "### 2.2 Cost function and Gradient\n",
        "\n",
        "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))$$\n",
        "* $m$ is the number of training examples\n",
        "* $y^{(i)}$ is the actual label of the i-th training example.\n",
        "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
        "\n",
        "The loss function for a single training example is:\n",
        "\n",
        "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
        "\n",
        "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a6e10c",
      "metadata": {
        "id": "e6a6e10c",
        "outputId": "8475607b-2916-427a-a6cb-3db379195b96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9.210340371976294"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
        "-1 * (1 - 0) * np.log(1 - 0.9999)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190491ea",
      "metadata": {
        "id": "190491ea"
      },
      "source": [
        "* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number: $-1 \\times log(0.0001) \\approx 9.2$.  The closer the prediction is to zero, the larger the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5a49638",
      "metadata": {
        "id": "f5a49638",
        "outputId": "0b82a956-b5af-4c9c-f573-060117aabceb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9.210340371976182"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
        "-1 * np.log(0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31da66e2",
      "metadata": {
        "id": "31da66e2"
      },
      "source": [
        "### 2.3 Implementing Gradient Descent\n",
        "\n",
        "To update weight vector $\\theta$, apply gradient descent to iteratively improve model's predictions.  \n",
        "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
        "\n",
        "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j$$\n",
        "* 'i' is the index across all 'm' training examples.\n",
        "* 'j' is the index of the weight $\\theta_j$, so $x^{(i)}_j$ is the feature associated with weight $\\theta_j$\n",
        "\n",
        "* To update the weight $\\theta_j$, adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
        "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
        "* The learning rate $\\alpha$ is a value that to control how big a single update will be.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eacdc2d",
      "metadata": {
        "id": "0eacdc2d"
      },
      "outputs": [],
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    '''\n",
        "\n",
        "    # 'm', the number of rows in matrix x\n",
        "    m = x.shape[0]\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "        \n",
        "        # z, the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "        \n",
        "        # the sigmoid of h\n",
        "        h = sigmoid(z)\n",
        "        \n",
        "        # calculate the cost function\n",
        "        J = -1./m *(np.dot(y.T, np.log(h))+np.dot((1-y).T,np.log(1-h)))                                                    \n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha/m) * np.dot(x.T,(h-y))\n",
        "        \n",
        "    J = float(J)\n",
        "    return J, theta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc56e8b9",
      "metadata": {
        "id": "cc56e8b9"
      },
      "source": [
        "### 2.4 Extracting the features\n",
        "\n",
        "* Given a list of tweets, extract two features and store them in a matrix.\n",
        "    * The first feature is the number of positive words in a tweet.\n",
        "    * The second feature is the number of negative words in a tweet. \n",
        "\n",
        "\n",
        "* The `extract_features()` function takes in a single tweet, process the tweet using the imported `process_tweet()` function and save the list of tweet words.\n",
        "\n",
        "    - For each word, check the `freqs` dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n",
        "    - Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b439f616",
      "metadata": {
        "id": "b439f616"
      },
      "outputs": [],
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output: \n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3)) \n",
        "    \n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1 \n",
        "    \n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "        \n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] += freqs.get((word, 1.0),0)\n",
        "        \n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] += freqs.get((word, 0.0),0)\n",
        "\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1909c7",
      "metadata": {
        "id": "7d1909c7",
        "outputId": "96a1bc0d-1dc4-494f-bac2-4fcdcd7d13ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.00e+00 3.02e+03 6.10e+01]]\n"
          ]
        }
      ],
      "source": [
        "# Testing `extract_features()` function\n",
        "sample_test = extract_features(train_x[0], freqs)\n",
        "print(sample_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11869468",
      "metadata": {
        "id": "11869468"
      },
      "source": [
        "### 2.5: Train Logistic Regression Model\n",
        "\n",
        "To train the model:\n",
        "* Stack the features for all training examples into a matrix `X`. \n",
        "* Call `gradientDescent()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b36053aa",
      "metadata": {
        "id": "b36053aa",
        "outputId": "0acd29ec-52ca-4d2e-cb81-d952689dcf72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cost after training is 0.24215613.\n",
            "The resulting vector of weights is [7e-08, 0.00052391, -0.00055517]\n"
          ]
        }
      ],
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6e7c5d",
      "metadata": {
        "id": "ea6e7c5d"
      },
      "source": [
        "### 2.6 Test Logistic Regression Model\n",
        "\n",
        "Predict whether a tweet is positive or negative.\n",
        "\n",
        "* The `predict_tweet()` function takes in a single tweet, process the tweet and gives the probability of a tweet being positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa10cc92",
      "metadata": {
        "id": "fa10cc92"
      },
      "outputs": [],
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output: \n",
        "        y_pred: the probability of a tweet being positive\n",
        "    '''\n",
        "    \n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet,freqs)\n",
        "    \n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "    \n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd0dc601",
      "metadata": {
        "id": "fd0dc601",
        "outputId": "dbf5df05-f9f3-46cb-fc8e-2fcd1fcd8e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am happy -> [[0.51858058]]\n",
            "I am bad -> [[0.49433913]]\n",
            "I lost my mobile :( -> [[0.11474572]]\n",
            "This movie should have been great -> [[0.51533146]]\n"
          ]
        }
      ],
      "source": [
        "# Testing `predict_tweet()` function\n",
        "for tweet in ['I am happy', \n",
        "              'I am bad',\n",
        "              'I lost my mobile :(',\n",
        "              'This movie should have been great']:\n",
        "    print(f'{tweet} -> {predict_tweet(tweet, freqs, theta)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7713c7f",
      "metadata": {
        "id": "f7713c7f"
      },
      "source": [
        "Check performance using the test set.\n",
        "\n",
        "* The `test_logistic_regression()` function takes test data and the weights of your trained model, calculate the accuracy of trained logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c718c4a",
      "metadata": {
        "id": "6c718c4a"
      },
      "outputs": [],
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output: \n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    \n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0)\n",
        "\n",
        "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    \n",
        "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab7f9da",
      "metadata": {
        "id": "9ab7f9da",
        "outputId": "d6a2e93e-468e-4cd1-ccc4-3f8081d5cc80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression model's accuracy = 0.9950\n"
          ]
        }
      ],
      "source": [
        "model_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {model_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d923c57d",
      "metadata": {
        "id": "d923c57d"
      },
      "source": [
        "### 2.7: Error Analysis\n",
        "\n",
        "In this part I will see some tweets that logistic regression model misclassified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bac8866",
      "metadata": {
        "id": "6bac8866",
        "outputId": "3380e735-8b64-4c8e-ee2c-13035b8e1218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR ANALYSIS\n",
            "\n",
            "\n",
            "THE TWEET IS: @jaredNOTsubway @iluvmariah @Bravotv Then that truly is a LATERAL move! Now, we all know the Queen Bee is UPWARD BOUND : ) #MovingOnUp\n",
            "\n",
            "THE PROCESSED TWEET IS: ['truli', 'later', 'move', 'know', 'queen', 'bee', 'upward', 'bound', 'movingonup']\n",
            "[1.]\t[[0.49996933]]\n",
            "\n",
            "\n",
            "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
            "\n",
            "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
            "[1.]\t[[0.48663882]]\n",
            "\n",
            "\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
            "http://t.co/UGQzOx0huu\n",
            "\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "[1.]\t[[0.48370724]]\n",
            "\n",
            "\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
            "\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "[1.]\t[[0.48370724]]\n",
            "\n",
            "\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
            "\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "[1.]\t[[0.48370724]]\n",
            "\n",
            "\n",
            "THE TWEET IS: off to the park to get some sunlight : )\n",
            "\n",
            "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
            "[1.]\t[[0.49578813]]\n",
            "\n",
            "\n",
            "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
            "\n",
            "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
            "[1.]\t[[0.48212925]]\n",
            "\n",
            "\n",
            "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
            "\n",
            "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
            "[0.]\t[[0.50020407]]\n",
            "\n",
            "\n",
            "THE TWEET IS: pats jay : (\n",
            "\n",
            "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
            "[0.]\t[[0.50039295]]\n",
            "\n",
            "\n",
            "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "\n",
            "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
            "[0.]\t[[0.50000002]]\n"
          ]
        }
      ],
      "source": [
        "print('ERROR ANALYSIS')\n",
        "for x,y in zip(test_x,test_y):\n",
        "    y_hat = predict_tweet(x, freqs, theta)\n",
        "\n",
        "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
        "        print(f'\\n\\nTHE TWEET IS: {x}\\n')\n",
        "        print(f'THE PROCESSED TWEET IS: {process_tweet(x)}')\n",
        "        print(f'{y}\\t{y_hat}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}