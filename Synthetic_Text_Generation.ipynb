{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "coursera": {
      "schema_names": [
        "NLPC3-2A"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayurshimbre123/mayurshimbre/blob/master/Synthetic_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jv7Y4hXwt0j"
      },
      "source": [
        "**Name** - Mayur Shimbre\n",
        "\n",
        "**Roll Number** - BE18B007\n",
        "\n",
        "**Institution** - Indian Institute of Technology Madras\n",
        "\n",
        "**Project Name** - Synthetic Text Generation Model using Recurrent Neural Network (RNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RVSwzQ5Bwt0m",
        "outputId": "9b51a13e-cf54-457f-e1ea-2574f9d67453"
      },
      "source": [
        "import os\n",
        "import trax\n",
        "import trax.fastmath.numpy as np\n",
        "import pickle\n",
        "import numpy\n",
        "import random as rnd\n",
        "from trax import fastmath\n",
        "from trax import layers as tl\n",
        "\n",
        "# set random seed\n",
        "trax.supervised.trainer_lib.init_random_number_generators(32)\n",
        "rnd.seed(32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sF9Hqzgwt0l"
      },
      "source": [
        "## 1: Importing the Data\n",
        "\n",
        "### 1.1 Loading in the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sDs36m81g6f"
      },
      "source": [
        "dirname = 'data/'\n",
        "lines = [] # storing all the lines in a variable. \n",
        "for filename in os.listdir(dirname):\n",
        "    with open(os.path.join(dirname, filename)) as files:\n",
        "        for line in files:\n",
        "            # remove leading and trailing whitespace\n",
        "            pure_line = line.strip()\n",
        "            \n",
        "            # if pure_line is not the empty string,\n",
        "            if pure_line:\n",
        "                # append it to the list\n",
        "                lines.append(pure_line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "-zMCe7aJkGwA",
        "outputId": "c0eace05-246a-47d9-9542-f009a4940836"
      },
      "source": [
        "n_lines = len(lines)\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of lines: 125097\n",
            "Sample line at position 0 A LOVER'S COMPLAINT\n",
            "Sample line at position 999 With this night's revels and expire the term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6XsiyHvkGwD"
      },
      "source": [
        "Notice that the letters are both uppercase and lowercase.  In order to reduce the complexity of the task, we will convert all characters to lowercase.  This way, the model only needs to predict the likelihood that a letter is 'a' and not decide between uppercase 'A' and lowercase 'a'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "UBO9jI8EkGwE",
        "outputId": "55b55d61-a5b1-4381-ff88-d146071ac671"
      },
      "source": [
        "# go through each line\n",
        "for i, line in enumerate(lines):\n",
        "    # convert to all lowercase\n",
        "    lines[i] = line.lower()\n",
        "\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of lines: 125097\n",
            "Sample line at position 0 a lover's complaint\n",
            "Sample line at position 999 with this night's revels and expire the term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygRkBeY0-SRD",
        "outputId": "3de4f6f8-7473-4c13-ea8d-cb2658f4964a"
      },
      "source": [
        "eval_lines = lines[-1000:] # Create a holdout validation set\n",
        "lines = lines[:-1000] # Leave the rest for training\n",
        "\n",
        "print(f\"Number of lines for training: {len(lines)}\")\n",
        "print(f\"Number of lines for validation: {len(eval_lines)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of lines for training: 124097\n",
            "Number of lines for validation: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDcxEmX31y3d"
      },
      "source": [
        "### 1.2 Convert a line to tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "Cc_B8ae3kGwI",
        "outputId": "94eb6798-827a-4494-dec0-7bb84523ce34"
      },
      "source": [
        "# View the unique unicode integer associated with each character\n",
        "print(f\"ord('a'): {ord('a')}\")\n",
        "print(f\"ord('b'): {ord('b')}\")\n",
        "print(f\"ord('c'): {ord('c')}\")\n",
        "print(f\"ord(' '): {ord(' ')}\")\n",
        "print(f\"ord('x'): {ord('x')}\")\n",
        "print(f\"ord('y'): {ord('y')}\")\n",
        "print(f\"ord('z'): {ord('z')}\")\n",
        "print(f\"ord('1'): {ord('1')}\")\n",
        "print(f\"ord('2'): {ord('2')}\")\n",
        "print(f\"ord('3'): {ord('3')}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ord('a'): 97\n",
            "ord('b'): 98\n",
            "ord('c'): 99\n",
            "ord(' '): 32\n",
            "ord('x'): 120\n",
            "ord('y'): 121\n",
            "ord('z'): 122\n",
            "ord('1'): 49\n",
            "ord('2'): 50\n",
            "ord('3'): 51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO4NSPkOITNK"
      },
      "source": [
        "def line_to_tensor(line, EOS_int=1):\n",
        "    \"\"\"Turns a line of text into a tensor\n",
        "\n",
        "    Args:\n",
        "        line (str): A single line of text.\n",
        "        EOS_int (int, optional): End-of-sentence integer. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        list: a list of integers (unicode values) for the characters in the `line`.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the tensor as an empty list\n",
        "    tensor = []\n",
        "\n",
        "    # for each character:\n",
        "    for c in line:\n",
        "        \n",
        "        # convert to unicode int\n",
        "        c_int = ord(c)\n",
        "        \n",
        "        # append the unicode integer to the tensor list\n",
        "        tensor.append(c_int)\n",
        "    \n",
        "    # include the end-of-sentence integer\n",
        "    tensor.append(EOS_int)\n",
        "\n",
        "    return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "D9Z_vtI7tTcw",
        "outputId": "0423ad21-af3e-4e6d-a558-472f4bf5f964"
      },
      "source": [
        "line_to_tensor('abc xyz')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[97, 98, 99, 32, 120, 121, 122, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFOR19cX2TQs"
      },
      "source": [
        "### 1.3 Batch generator \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMingz5xzrGD"
      },
      "source": [
        "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
        "    \"\"\"Generator function that yields batches of data\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
        "        max_length (int): maximum length of the output tensor.\n",
        "        NOTE: max_length includes the end-of-sentence character that will be added\n",
        "                to the tensor.  \n",
        "                Keep in mind that the length of the tensor is always 1 + the length\n",
        "                of the original line of characters.\n",
        "        data_lines (list): list of the sentences to group into batches.\n",
        "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
        "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
        "\n",
        "    Yields:\n",
        "        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).\n",
        "        NOTE: jax.interpreters.xla.DeviceArray is trax's version of numpy.ndarray\n",
        "    \"\"\"\n",
        "    # initialize the index that points to the current position in the lines index array\n",
        "    index = 0\n",
        "    \n",
        "    # initialize the list that will contain the current batch\n",
        "    cur_batch = []\n",
        "    \n",
        "    # count the number of lines in data_lines\n",
        "    num_lines = len(data_lines)\n",
        "    \n",
        "    # create an array with the indexes of data_lines that can be shuffled\n",
        "    lines_index = [*range(num_lines)]\n",
        "    \n",
        "    # shuffle line indexes if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(lines_index)\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        # if the index is greater or equal than to the number of lines in data_lines\n",
        "        if index >= num_lines:\n",
        "            # then reset the index to 0\n",
        "            index = 0\n",
        "            # shuffle line indexes if shuffle is set to True\n",
        "            if shuffle:\n",
        "                rnd.shuffle(lines_index)\n",
        "            \n",
        "        # get a line at the `lines_index[index]` position in data_lines\n",
        "        line = data_lines[lines_index[index]]\n",
        "        \n",
        "        # if the length of the line is less than max_length\n",
        "        if len(line) < max_length:\n",
        "            # append the line to the current batch\n",
        "            cur_batch.append(line)\n",
        "            \n",
        "        # increment the index by one\n",
        "        index += 1\n",
        "        \n",
        "        # if the current batch is now equal to the desired batch size\n",
        "        if len(cur_batch) == batch_size:\n",
        "            \n",
        "            batch = []\n",
        "            mask = []\n",
        "            \n",
        "            # go through each line (li) in cur_batch\n",
        "            for li in cur_batch:\n",
        "                # convert the line (li) to a tensor of integers\n",
        "                tensor = line_to_tensor(li)\n",
        "                \n",
        "                # Create a list of zeros to represent the padding\n",
        "                # so that the tensor plus padding will have length `max_length`\n",
        "                pad = [0] * (max_length - len(tensor))\n",
        "                \n",
        "                # combine the tensor plus pad\n",
        "                tensor_pad = tensor + pad\n",
        "                \n",
        "                # append the padded tensor to the batch\n",
        "                batch.append(tensor_pad)\n",
        "\n",
        "                # A mask for  tensor_pad is 1 wherever tensor_pad is not\n",
        "                # 0 and 0 wherever tensor_pad is 0, i.e. if tensor_pad is\n",
        "                # [1, 2, 3, 0, 0, 0] then example_mask should be\n",
        "                # [1, 1, 1, 0, 0, 0]\n",
        "                example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
        "                mask.append(example_mask)\n",
        "               \n",
        "            # convert the batch (data type list) to a trax's numpy array\n",
        "            batch_np_arr = np.array(batch)\n",
        "            mask_np_arr = np.array(mask)\n",
        "            \n",
        "            # Yield two copies of the batch and mask.\n",
        "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
        "            \n",
        "            # reset the current batch to an empty list\n",
        "            cur_batch = []\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "XJ1HBoEHwt0p",
        "scrolled": true,
        "outputId": "4fbe7f04-5ce9-49c5-f1dd-0136cc28a114"
      },
      "source": [
        "tmp_lines = ['12345678901', #length 11\n",
        "             '123456789', # length 9\n",
        "             '234567890', # length 9\n",
        "             '345678901'] # length 9\n",
        "\n",
        "# Get a batch size of 2, max length 10\n",
        "tmp_data_gen = data_generator(batch_size=2, \n",
        "                              max_length=10, \n",
        "                              data_lines=tmp_lines,\n",
        "                              shuffle=False)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_data_gen)\n",
        "\n",
        "# view the batch\n",
        "tmp_batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
              "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
              " DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
              "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
              " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "              [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFcB-i-rDd68"
      },
      "source": [
        "### 1.4 Repeating Batch generator \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v589leeZETy7"
      },
      "source": [
        "import itertools\n",
        "\n",
        "infinite_data_generator = itertools.cycle(\n",
        "    data_generator(batch_size=2, max_length=10, data_lines=tmp_lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0lJhBPgJFAxb",
        "outputId": "0849fa48-0d82-4050-b3b4-e738d96a7ca8"
      },
      "source": [
        "ten_lines = [next(infinite_data_generator) for _ in range(10)]\n",
        "print(len(ten_lines))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmZRBoaMwt0w"
      },
      "source": [
        "\n",
        "## 2: Defining the GRU model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hww76f8_wt0x"
      },
      "source": [
        "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
        "    \"\"\"Returns a GRU language model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
        "        d_model (int, optional): Depth of embedding (n_units in the GRU cell). Defaults to 512.\n",
        "        n_layers (int, optional): Number of GRU layers. Defaults to 2.\n",
        "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: A GRU language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
        "    \"\"\"\n",
        "\n",
        "    model = tl.Serial(\n",
        "      tl.ShiftRight(mode=mode), # Layer that adds padding to shift the input\n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Stack the embedding layer\n",
        "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units with n_layer parameter \n",
        "      tl.Dense(n_units=vocab_size), # Dense layer\n",
        "      tl.LogSoftmax() # Log Softmax Layer\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "kvQ_jf52-JAn",
        "outputId": "9d13c577-f89d-427a-8944-a00ca57a4f2c"
      },
      "source": [
        "# testing model\n",
        "model = GRULM()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  ShiftRight(1)\n",
            "  Embedding_256_512\n",
            "  GRU_512\n",
            "  GRU_512\n",
            "  Dense_256\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsvjaCQ6wt02"
      },
      "source": [
        "# 3: Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Birerv82mLv"
      },
      "source": [
        "batch_size = 32\n",
        "max_length = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "T3NxHd-VtTcb",
        "outputId": "b945b175-3101-4600-ac2f-8c705c13d752"
      },
      "source": [
        "def n_used_lines(lines, max_length):\n",
        "    '''\n",
        "    Args: \n",
        "    lines: all lines of text an array of lines\n",
        "    max_length - max_length of a line in order to be considered an int\n",
        "    output_dir - folder to save your file an int\n",
        "    Return:\n",
        "    number of efective examples\n",
        "    '''\n",
        "\n",
        "    n_lines = 0\n",
        "    for l in lines:\n",
        "        if len(l) <= max_length:\n",
        "            n_lines += 1\n",
        "    return n_lines\n",
        "\n",
        "num_used_lines = n_used_lines(lines, 32)\n",
        "print('Number of used lines from the dataset:', num_used_lines)\n",
        "print('Batch size (a power of 2):', int(batch_size))\n",
        "steps_per_epoch = int(num_used_lines/batch_size)\n",
        "print('Number of steps to cover one epoch:', steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of used lines from the dataset: 25881\n",
            "Batch size (a power of 2): 32\n",
            "Number of steps to cover one epoch: 808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgFMfH5awt07"
      },
      "source": [
        "### 3.1 Training the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kbtfz4T_m7x"
      },
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "def train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=1, output_dir='model/'): \n",
        "    \"\"\"Function that trains the model\n",
        "\n",
        "    Args:\n",
        "        model (trax.layers.combinators.Serial): GRU model.\n",
        "        data_generator (function): Data generator function.\n",
        "        batch_size (int, optional): Number of lines per batch. Defaults to 32.\n",
        "        max_length (int, optional): Maximum length allowed for a line to be processed. Defaults to 64.\n",
        "        lines (list, optional): List of lines to use for training. Defaults to lines.\n",
        "        eval_lines (list, optional): List of lines to use for evaluation. Defaults to eval_lines.\n",
        "        n_steps (int, optional): Number of steps to train. Defaults to 1.\n",
        "        output_dir (str, optional): Relative path of directory to save model. Defaults to \"model/\".\n",
        "\n",
        "    Returns:\n",
        "        trax.supervised.training.Loop: Training loop for the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
        "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
        "    \n",
        "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
        "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
        "   \n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=infinite_train_generator, # Use infinite train data generator\n",
        "        loss_layer=tl.CrossEntropyLoss(),   # Don't forget to instantiate this object\n",
        "        optimizer=trax.optimizers.Adam(0.0005)     # Don't forget to add the learning rate parameter\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=infinite_eval_generator,    # Use infinite eval data generator\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], # Don't forget to instantiate these objects\n",
        "        n_eval_batches=3      # For better evaluation accuracy in reasonable time\n",
        "    )\n",
        "    \n",
        "    training_loop = training.Loop(model,\n",
        "                                  train_task,\n",
        "                                  eval_task=eval_task,\n",
        "                                  output_dir=output_dir)\n",
        "\n",
        "    training_loop.run(n_steps=n_steps)\n",
        "    \n",
        "    \n",
        "    # I return this because it contains a handle to the model, which has the weights etc.\n",
        "    return training_loop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "SwP646GpK3pD",
        "outputId": "4c88bcf5-a8aa-4160-cc3c-3ff16f39fd64"
      },
      "source": [
        "# Train the model 1 step and keep the `trax.supervised.training.Loop` object.\n",
        "training_loop = train_model(GRULM(), data_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step      1: train CrossEntropyLoss |  5.54486227\n",
            "Step      1: eval  CrossEntropyLoss |  5.48863840\n",
            "Step      1: eval          Accuracy |  0.17598992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abKPe7d4wt1C"
      },
      "source": [
        "# Part 4:  Evaluation  \n",
        "\n",
        "### 4.1 Evaluating using the deep nets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OtmlEuOwt1D"
      },
      "source": [
        "def test_model(preds, target):\n",
        "    \"\"\"Function to test the model.\n",
        "\n",
        "    Args:\n",
        "        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.\n",
        "        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.\n",
        "\n",
        "    Returns:\n",
        "        float: log_perplexity of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    total_log_ppx = np.sum(preds * tl.one_hot(target, preds.shape[-1]),axis= -1) # HINT: tl.one_hot() should replace one of the Nones\n",
        "\n",
        "    non_pad = 1.0 - np.equal(target, 0)          # You should check if the target equals 0\n",
        "    ppx = total_log_ppx * non_pad                       # Get rid of the padding\n",
        "\n",
        "    log_ppx = np.sum(ppx) / np.sum(non_pad)\n",
        "    \n",
        "    return -log_ppx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xl8X0FPAwt1F",
        "outputId": "1dbfef92-c8ca-4cae-c92c-7b1b4adb6963"
      },
      "source": [
        "# Testing \n",
        "model = GRULM()\n",
        "model.init_from_file('model.pkl.gz')\n",
        "batch = next(data_generator(batch_size, max_length, lines, shuffle=False))\n",
        "preds = model(batch[0])\n",
        "log_ppx = test_model(preds, batch[1])\n",
        "print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The log perplexity and perplexity of your model are respectively 1.9785146 7.2319922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-STC44Ywt1I"
      },
      "source": [
        "# 5: Generating the language with your own model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xrOJHbXewt1J",
        "outputId": "665bc6ff-f9ee-4097-c89b-83ef0e12c1b7"
      },
      "source": [
        "def gumbel_sample(log_probs, temperature=1.0):\n",
        "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
        "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
        "    g = -np.log(-np.log(u))\n",
        "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
        "\n",
        "def predict(num_chars, prefix):\n",
        "    inp = [ord(c) for c in prefix]\n",
        "    result = [c for c in prefix]\n",
        "    max_len = len(prefix) + num_chars\n",
        "    for _ in range(num_chars):\n",
        "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
        "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
        "        next_char = gumbel_sample(outp[0, len(inp)])\n",
        "        inp += [int(next_char)]\n",
        "       \n",
        "        if inp[-1] == 1:\n",
        "            break  # EOS\n",
        "        result.append(chr(int(next_char)))\n",
        "    \n",
        "    return \"\".join(result)\n",
        "\n",
        "print(predict(32, \"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And in the shapes of heaven, he \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "Raojyhw3z7HE",
        "outputId": "e8e88711-e200-462c-f984-018dfa784a63"
      },
      "source": [
        "print(predict(32, \"\"))\n",
        "print(predict(32, \"\"))\n",
        "print(predict(32, \"\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MARK ANTONY\tTo go, good sir.\n",
            "Even with a countenance, exempt \n",
            "I'll leave him to; so 'twere so \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAfV3l5Zwt1L"
      },
      "source": [
        "In the generated text above, you can see that the model generates text that makes sense capturing dependencies between words and without any input. A simple n-gram model would have not been able to capture all of that in one sentence."
      ]
    }
  ]
}